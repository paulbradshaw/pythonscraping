# Session 2 tasks:

Create and share a new notebook which does the following:

* Imports the libraries you need to scrape a webpage and store the info from it
* Use the `scraperwiki` library to scrape a webpage from a URL
* Use the `lxml.html` library to convert it to an lxml 'object'
* Use CSS selectors to drill down into that object and extract info from the webpage (see note below)
* Use `pandas` to create a data frame and store that info
* Export it as a CSV
* Download it to your computer

Note: if you don't seem to be getting the info that you can see on the webpage, [follow the steps in this guide](https://onlinejournalismblog.com/2017/05/10/how-to-find-data-behind-chart-map-using-inspector/) to see if it's being dynamically generated (which would explain why your scraper can't see it). In that situation you may be able to get the information another way, but try scraping a different page instead so you get to practise your Python.

As always, be prepared to play and experiment and take risks - nothing terrible is going to happen if something doesn't work. If you get an error that's fine - try googling it to see if you can make sense of it at all (you might need to try a few links, and something it still doesn't make sense anyway)
